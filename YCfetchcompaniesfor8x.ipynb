{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTu6d+6hdsJCQchfPY2b4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akhmtvaae/CurveSwitch/blob/master/YCfetchcompaniesfor8x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRg-RRy0Ianf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "yc_8x_icp_funded_to_excel.py\n",
        "\n",
        "Exports YC companies to Excel and prioritizes those that are good fits for 8x:\n",
        "- consumer/performance-marketing heavy categories (fintech, consumer apps, DTC/ecom, beauty, fem health, creator tools)\n",
        "- recent batch/hiring signals\n",
        "- funding signal from \"Latest News\" on the YC company page (best-effort)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import asyncio\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timezone\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import httpx\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential_jitter\n",
        "\n",
        "\n",
        "ALL_COMPANIES_URL = \"https://yc-oss.github.io/api/companies/all.json\"\n",
        "YC_COMPANY_URL_TEMPLATE = \"https://www.ycombinator.com/companies/{slug}\"\n",
        "\n",
        "USER_AGENT = (\n",
        "    \"Mozilla/5.0 (compatible; 8x-yc-sourcing/1.0; +https://www.ycombinator.com/companies)\"\n",
        ")\n",
        "\n",
        "# -------- Heuristics --------\n",
        "FUNDING_KEYWORDS = [\n",
        "    \"raises\", \"raised\", \"funding\", \"series a\", \"series b\", \"series c\",\n",
        "    \"seed\", \"round\", \"financing\", \"investment\", \"led by\"\n",
        "]\n",
        "\n",
        "ARR_REGEXES = [\n",
        "    re.compile(r\"\\b(\\$?\\d+(?:\\.\\d+)?\\s?(?:k|m|b))\\s*arr\\b\", re.IGNORECASE),\n",
        "    re.compile(r\"\\barr\\s*[:\\-]?\\s*(\\$?\\d+(?:\\.\\d+)?\\s?(?:k|m|b))\\b\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "MONEY_REGEX = re.compile(r\"(\\$|€|£)\\s?\\d+(?:\\.\\d+)?\\s?(?:k|m|b)\\b\", re.IGNORECASE)\n",
        "\n",
        "# ICP keyword buckets (tune freely)\n",
        "ICP_BUCKETS = {\n",
        "    \"Fintech\": [\n",
        "        \"fintech\", \"bank\", \"banking\", \"card\", \"credit\", \"debit\", \"payments\", \"wallet\",\n",
        "        \"lending\", \"loan\", \"remittance\", \"insurance\", \"wealth\", \"invest\", \"brokerage\"\n",
        "    ],\n",
        "    \"Consumer App\": [\n",
        "        \"app\", \"mobile\", \"subscription\", \"consumer\", \"productivity\", \"photo\", \"video\", \"ai\",\n",
        "        \"coach\", \"planner\"\n",
        "    ],\n",
        "    \"Creator/Video Tools\": [\n",
        "        \"creator\", \"video\", \"editing\", \"caption\", \"clip\", \"ugc\", \"content\", \"social\", \"tiktok\", \"instagram\"\n",
        "    ],\n",
        "    \"DTC/Ecom/Marketplace\": [\n",
        "        \"ecommerce\", \"e-commerce\", \"shop\", \"shopping\", \"marketplace\", \"brand\", \"direct-to-consumer\",\n",
        "        \"dtc\", \"store\", \"checkout\"\n",
        "    ],\n",
        "    \"Beauty/Skincare\": [\n",
        "        \"beauty\", \"skincare\", \"skin\", \"derm\", \"acne\", \"routine\", \"cosmetics\", \"hair\"\n",
        "    ],\n",
        "    \"Women’s Health/Femtech\": [\n",
        "        \"women\", \"female\", \"menopause\", \"cycle\", \"period\", \"fertility\", \"pregnancy\",\n",
        "        \"postpartum\", \"pelvic\", \"hormone\", \"femtech\"\n",
        "    ],\n",
        "    \"Health/Wellness\": [\n",
        "        \"health\", \"wellness\", \"care\", \"clinic\", \"therapy\", \"telehealth\", \"medical\", \"patient\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Business-model / paid-social fit hints\n",
        "PAID_SOCIAL_FIT_HINTS = [\n",
        "    \"subscription\", \"consumer\", \"app\", \"marketplace\", \"brand\", \"shop\", \"card\",\n",
        "    \"credit\", \"bank\", \"beauty\", \"skincare\", \"women\", \"health\"\n",
        "]\n",
        "\n",
        "\n",
        "def now_utc() -> datetime:\n",
        "    return datetime.now(timezone.utc)\n",
        "\n",
        "\n",
        "def normalize_text(s: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", (s or \"\")).strip()\n",
        "\n",
        "\n",
        "def safe_join(items: Any) -> str:\n",
        "    if items is None:\n",
        "        return \"\"\n",
        "    if isinstance(items, list):\n",
        "        return \"; \".join(str(x) for x in items if x is not None)\n",
        "    return str(items)\n",
        "\n",
        "\n",
        "def parse_batch_to_date(batch: str) -> Optional[datetime]:\n",
        "    if not batch:\n",
        "        return None\n",
        "    b = batch.strip()\n",
        "\n",
        "    m = re.fullmatch(r\"([WSF])(\\d{2})\", b)\n",
        "    if m:\n",
        "        season_code, yy = m.group(1), int(m.group(2))\n",
        "        year = 2000 + yy\n",
        "        month = {\"W\": 1, \"S\": 7, \"F\": 10}[season_code]\n",
        "        return datetime(year, month, 1, tzinfo=timezone.utc)\n",
        "\n",
        "    m = re.fullmatch(r\"(Winter|Spring|Summer|Fall)\\s+(\\d{4})\", b, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        season, year = m.group(1).lower(), int(m.group(2))\n",
        "        month = {\"winter\": 1, \"spring\": 4, \"summer\": 7, \"fall\": 10}[season]\n",
        "        return datetime(year, month, 1, tzinfo=timezone.utc)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def months_since(dt: Optional[datetime]) -> Optional[float]:\n",
        "    if not dt:\n",
        "        return None\n",
        "    delta = now_utc() - dt\n",
        "    return delta.days / 30.44\n",
        "\n",
        "\n",
        "def blob_for_company(c: Dict[str, Any]) -> str:\n",
        "    parts = [\n",
        "        c.get(\"name\", \"\"),\n",
        "        c.get(\"one_liner\", \"\"),\n",
        "        c.get(\"long_description\", \"\"),\n",
        "        c.get(\"industry\", \"\"),\n",
        "        c.get(\"subindustry\", \"\"),\n",
        "        safe_join(c.get(\"tags\")),\n",
        "    ]\n",
        "    return normalize_text(\" \".join(parts)).lower()\n",
        "\n",
        "\n",
        "def detect_funding_signal(*texts: str) -> bool:\n",
        "    blob = \" \".join(normalize_text(t).lower() for t in texts if t)\n",
        "    return any(k in blob for k in FUNDING_KEYWORDS)\n",
        "\n",
        "\n",
        "def extract_arr_value(*texts: str) -> str:\n",
        "    blob = \" \".join(normalize_text(t) for t in texts if t)\n",
        "    for rx in ARR_REGEXES:\n",
        "        m = rx.search(blob)\n",
        "        if m:\n",
        "            return m.group(1).strip()\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def extract_money_mentions(*texts: str) -> str:\n",
        "    blob = \" \".join(normalize_text(t) for t in texts if t)\n",
        "    matches = [m.group(0) for m in MONEY_REGEX.finditer(blob)]\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in matches:\n",
        "        k = x.lower().strip()\n",
        "        if k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(x.strip())\n",
        "    return \"; \".join(out)\n",
        "\n",
        "\n",
        "def pick_icp_bucket(c: Dict[str, Any]) -> Tuple[str, int]:\n",
        "    \"\"\"\n",
        "    Returns (bucket_name, bucket_score). 'Other' if no strong match.\n",
        "    \"\"\"\n",
        "    text = blob_for_company(c)\n",
        "    best_bucket = \"Other\"\n",
        "    best_score = 0\n",
        "    for bucket, kws in ICP_BUCKETS.items():\n",
        "        score = 0\n",
        "        for kw in kws:\n",
        "            if kw in text:\n",
        "                score += 1\n",
        "        if score > best_score:\n",
        "            best_bucket, best_score = bucket, score\n",
        "\n",
        "    # Convert keyword hits into a score weight\n",
        "    # (cap so a long description doesn't dominate)\n",
        "    bucket_points = min(best_score, 8)\n",
        "    return best_bucket, bucket_points\n",
        "\n",
        "\n",
        "def icp_filter_ok(c: Dict[str, Any], min_bucket_hits: int) -> bool:\n",
        "    bucket, hits = pick_icp_bucket(c)\n",
        "    if bucket == \"Other\":\n",
        "        return False\n",
        "    return hits >= min_bucket_hits\n",
        "\n",
        "\n",
        "def pre_points(c: Dict[str, Any]) -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Lightweight scoring before enrichment.\n",
        "    \"\"\"\n",
        "    pts = 0\n",
        "    reasons = []\n",
        "\n",
        "    status = (c.get(\"status\") or \"\").lower()\n",
        "    if status == \"active\":\n",
        "        pts += 10\n",
        "        reasons.append(\"active(+10)\")\n",
        "\n",
        "    stage = (c.get(\"stage\") or \"\").lower()\n",
        "    if stage == \"early\":\n",
        "        pts += 8\n",
        "        reasons.append(\"early(+8)\")\n",
        "    elif stage == \"growth\":\n",
        "        pts += 6\n",
        "        reasons.append(\"growth(+6)\")\n",
        "\n",
        "    if c.get(\"isHiring\") is True:\n",
        "        pts += 5\n",
        "        reasons.append(\"hiring(+5)\")\n",
        "\n",
        "    # recency via batch\n",
        "    bdt = parse_batch_to_date(c.get(\"batch\") or \"\")\n",
        "    ms = months_since(bdt)\n",
        "    if ms is not None:\n",
        "        if ms <= 6:\n",
        "            pts += 12\n",
        "            reasons.append(\"batch<=6mo(+12)\")\n",
        "        elif ms <= 12:\n",
        "            pts += 10\n",
        "            reasons.append(\"batch<=12mo(+10)\")\n",
        "        elif ms <= 24:\n",
        "            pts += 7\n",
        "            reasons.append(\"batch<=24mo(+7)\")\n",
        "        elif ms <= 48:\n",
        "            pts += 3\n",
        "            reasons.append(\"batch<=48mo(+3)\")\n",
        "\n",
        "    # ICP bucket fit\n",
        "    bucket, bucket_hits = pick_icp_bucket(c)\n",
        "    if bucket != \"Other\":\n",
        "        add = 2 * bucket_hits  # amplify relevance\n",
        "        pts += add\n",
        "        reasons.append(f\"{bucket}(+{add})\")\n",
        "\n",
        "    # paid-social fit hints\n",
        "    text = blob_for_company(c)\n",
        "    hint_hits = sum(1 for h in PAID_SOCIAL_FIT_HINTS if h in text)\n",
        "    add = min(hint_hits, 6)\n",
        "    pts += add\n",
        "    if add:\n",
        "        reasons.append(f\"paid_fit_hints(+{add})\")\n",
        "\n",
        "    # team size sweet spot\n",
        "    ts = c.get(\"team_size\")\n",
        "    if isinstance(ts, int):\n",
        "        if 15 <= ts <= 250:\n",
        "            pts += 4\n",
        "            reasons.append(\"team15-250(+4)\")\n",
        "        elif ts > 250:\n",
        "            pts += 2\n",
        "            reasons.append(\"team>250(+2)\")\n",
        "\n",
        "    return pts, \" | \".join(reasons)\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential_jitter(initial=0.5, max=6))\n",
        "async def fetch_text(client: httpx.AsyncClient, url: str) -> str:\n",
        "    r = await client.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_exponential_jitter(initial=0.5, max=6))\n",
        "async def fetch_json(client: httpx.AsyncClient, url: str) -> Any:\n",
        "    r = await client.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "\n",
        "def looks_like_name(s: str) -> bool:\n",
        "    s = normalize_text(s)\n",
        "    if len(s) < 3 or len(s) > 60:\n",
        "        return False\n",
        "    return bool(re.match(r\"^[A-Z][A-Za-z'\\-]+(\\s+[A-Z][A-Za-z'\\-]+)+$\", s))\n",
        "\n",
        "\n",
        "def parse_company_page(html: str) -> Dict[str, Any]:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "    # Latest News (best effort)\n",
        "    news_items: List[Dict[str, str]] = []\n",
        "    latest_news_header = soup.find(string=re.compile(r\"\\bLatest News\\b\", re.IGNORECASE))\n",
        "    if latest_news_header:\n",
        "        node = latest_news_header.parent\n",
        "        anchors = []\n",
        "        for a in node.find_all_next(\"a\", href=True, limit=60):\n",
        "            title = normalize_text(a.get_text(\" \", strip=True))\n",
        "            href = a.get(\"href\", \"\")\n",
        "            if title and href:\n",
        "                anchors.append((title, href))\n",
        "        # keep first few distinct titles\n",
        "        seen = set()\n",
        "        for title, href in anchors:\n",
        "            tkey = title.lower()\n",
        "            if tkey in seen:\n",
        "                continue\n",
        "            seen.add(tkey)\n",
        "            # news links are often external; but keep anyway\n",
        "            news_items.append({\"title\": title, \"url\": href})\n",
        "            if len(news_items) >= 6:\n",
        "                break\n",
        "\n",
        "    # Founders (best effort)\n",
        "    founders: List[Dict[str, str]] = []\n",
        "    founders_header = soup.find(string=re.compile(r\"\\bActive Founders\\b|\\bFounders\\b\", re.IGNORECASE))\n",
        "    if founders_header:\n",
        "        header_tag = founders_header.parent\n",
        "        stop_markers = re.compile(r\"\\bLatest News\\b|\\bCompany Launches\\b\", re.IGNORECASE)\n",
        "\n",
        "        for a in header_tag.find_all_next(\"a\", href=True, limit=220):\n",
        "            href = a.get(\"href\", \"\")\n",
        "            txt = normalize_text(a.get_text(\" \", strip=True))\n",
        "            if stop_markers.search(txt):\n",
        "                break\n",
        "            if \"linkedin.com\" not in href:\n",
        "                continue\n",
        "\n",
        "            # Search backwards for a human name\n",
        "            name = \"\"\n",
        "            for prev in a.find_all_previous(string=True, limit=35):\n",
        "                cand = normalize_text(prev)\n",
        "                if looks_like_name(cand):\n",
        "                    name = cand\n",
        "                    break\n",
        "\n",
        "            if name:\n",
        "                founders.append({\"name\": name, \"linkedin\": href, \"role\": \"Founder\"})\n",
        "\n",
        "        # de-dupe\n",
        "        dedup = {}\n",
        "        for f in founders:\n",
        "            dedup[(f[\"name\"].lower(), f[\"linkedin\"])] = f\n",
        "        founders = list(dedup.values())\n",
        "\n",
        "    company_linkedin = \"\"\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if \"linkedin.com/company\" in href:\n",
        "            company_linkedin = href\n",
        "            break\n",
        "\n",
        "    return {\"news\": news_items, \"founders\": founders, \"company_linkedin\": company_linkedin}\n",
        "\n",
        "\n",
        "def pick_point_of_contact(enriched: Dict[str, Any]) -> Tuple[str, str, str]:\n",
        "    founders = (enriched or {}).get(\"founders\") or []\n",
        "    if founders:\n",
        "        f0 = founders[0]\n",
        "        return f0.get(\"name\", \"\"), f0.get(\"role\", \"Founder\"), f0.get(\"linkedin\", \"\")\n",
        "    return \"\", \"\", \"\"\n",
        "\n",
        "\n",
        "def post_points(\n",
        "    c: Dict[str, Any],\n",
        "    funding_signal: bool,\n",
        "    bucket: str,\n",
        "    bucket_hits: int,\n",
        ") -> Tuple[int, str]:\n",
        "    \"\"\"\n",
        "    Final points after enrichment + funding signal.\n",
        "    \"\"\"\n",
        "    pts, reasons = pre_points(c)\n",
        "    reasons_list = [reasons] if reasons else []\n",
        "\n",
        "    # Reward funding signal heavily if funded-only sourcing\n",
        "    if funding_signal:\n",
        "        pts += 12\n",
        "        reasons_list.append(\"funding_signal(+12)\")\n",
        "\n",
        "    # Extra bump for specific high-value buckets\n",
        "    if bucket in {\"Fintech\", \"DTC/Ecom/Marketplace\", \"Beauty/Skincare\", \"Women’s Health/Femtech\"}:\n",
        "        pts += 6\n",
        "        reasons_list.append(f\"{bucket}_priority(+6)\")\n",
        "    elif bucket in {\"Creator/Video Tools\", \"Consumer App\"}:\n",
        "        pts += 4\n",
        "        reasons_list.append(f\"{bucket}_priority(+4)\")\n",
        "\n",
        "    return pts, \" | \".join([r for r in reasons_list if r])\n",
        "\n",
        "\n",
        "async def enrich_batch(\n",
        "    companies: List[Dict[str, Any]],\n",
        "    concurrency: int,\n",
        "    sleep_min: float,\n",
        "    sleep_max: float,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    sem = asyncio.Semaphore(concurrency)\n",
        "\n",
        "    async with httpx.AsyncClient(headers={\"User-Agent\": USER_AGENT, \"Accept-Language\": \"en\"}) as client:\n",
        "\n",
        "        async def enrich_one(c: Dict[str, Any]) -> Dict[str, Any]:\n",
        "            slug = c.get(\"slug\", \"\")\n",
        "            url = YC_COMPANY_URL_TEMPLATE.format(slug=slug)\n",
        "            async with sem:\n",
        "                await asyncio.sleep(random.uniform(sleep_min, sleep_max))\n",
        "                try:\n",
        "                    html = await fetch_text(client, url)\n",
        "                    parsed = parse_company_page(html)\n",
        "                except Exception:\n",
        "                    parsed = {\"news\": [], \"founders\": [], \"company_linkedin\": \"\"}\n",
        "            c[\"_enriched\"] = parsed\n",
        "            return c\n",
        "\n",
        "        return await asyncio.gather(*[enrich_one(c) for c in companies])\n",
        "\n",
        "\n",
        "def flatten_row(c: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    enriched = c.get(\"_enriched\", {}) or {}\n",
        "    news = enriched.get(\"news\") or []\n",
        "    founders = enriched.get(\"founders\") or []\n",
        "\n",
        "    news_titles = \"; \".join(n.get(\"title\", \"\") for n in news if n.get(\"title\"))\n",
        "    news_urls = \"; \".join(n.get(\"url\", \"\") for n in news if n.get(\"url\"))\n",
        "\n",
        "    funding_signal = detect_funding_signal(c.get(\"one_liner\", \"\"), c.get(\"long_description\", \"\"), news_titles)\n",
        "    arr_val = extract_arr_value(c.get(\"long_description\", \"\"), news_titles)\n",
        "    money_mentions = extract_money_mentions(c.get(\"long_description\", \"\"), news_titles)\n",
        "\n",
        "    bucket, bucket_hits = pick_icp_bucket(c)\n",
        "    points, points_breakdown = post_points(c, funding_signal, bucket, bucket_hits)\n",
        "\n",
        "    poc_name, poc_role, poc_linkedin = pick_point_of_contact(enriched)\n",
        "\n",
        "    return {\n",
        "        \"name\": c.get(\"name\"),\n",
        "        \"slug\": c.get(\"slug\"),\n",
        "        \"batch\": c.get(\"batch\"),\n",
        "        \"status\": c.get(\"status\"),\n",
        "        \"stage\": c.get(\"stage\"),\n",
        "        \"industry\": c.get(\"industry\"),\n",
        "        \"subindustry\": c.get(\"subindustry\"),\n",
        "        \"tags\": safe_join(c.get(\"tags\")),\n",
        "        \"team_size\": c.get(\"team_size\"),\n",
        "        \"is_hiring\": c.get(\"isHiring\"),\n",
        "        \"locations\": c.get(\"all_locations\"),\n",
        "        \"website\": c.get(\"website\"),\n",
        "        \"yc_url\": c.get(\"url\"),\n",
        "\n",
        "        \"icp_bucket\": bucket,\n",
        "        \"raised_funding_signal\": funding_signal,\n",
        "        \"funding_money_mentions\": money_mentions,\n",
        "        \"arr_mentioned\": bool(arr_val),\n",
        "        \"arr_value\": arr_val,\n",
        "\n",
        "        \"vector\": f\"{bucket} | {c.get('industry','')} | {c.get('subindustry','')} | {safe_join(c.get('tags'))}\",\n",
        "        \"points\": points,\n",
        "        \"points_breakdown\": points_breakdown,\n",
        "\n",
        "        \"company_linkedin\": enriched.get(\"company_linkedin\", \"\"),\n",
        "        \"founders\": \"; \".join(f.get(\"name\", \"\") for f in founders if f.get(\"name\")),\n",
        "\n",
        "        \"poc_name\": poc_name,\n",
        "        \"poc_role\": poc_role,\n",
        "        \"poc_linkedin\": poc_linkedin,\n",
        "\n",
        "        \"latest_news_titles\": news_titles,\n",
        "        \"latest_news_urls\": news_urls,\n",
        "    }\n",
        "\n",
        "\n",
        "async def main() -> None:\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--limit\", type=int, default=1000)\n",
        "    ap.add_argument(\"--out\", type=str, default=\"yc_8x_1000.xlsx\")\n",
        "    ap.add_argument(\"--concurrency\", type=int, default=5)\n",
        "    ap.add_argument(\"--sleep-min\", type=float, default=0.25)\n",
        "    ap.add_argument(\"--sleep-max\", type=float, default=0.75)\n",
        "\n",
        "    ap.add_argument(\"--active-only\", action=\"store_true\", help=\"Only keep status=Active\")\n",
        "    ap.add_argument(\"--funded-only\", action=\"store_true\", help=\"Only keep companies with funding signal from YC page news\")\n",
        "    ap.add_argument(\"--prefetch\", type=int, default=6000, help=\"How many top candidates to enrich before filtering down\")\n",
        "    ap.add_argument(\"--icp-only\", action=\"store_true\", help=\"Filter to 8x ICP buckets only\")\n",
        "    ap.add_argument(\"--min-icp-hits\", type=int, default=1, help=\"Min keyword hits to be considered ICP\")\n",
        "    ap.add_argument(\"--recent-batches-months\", type=int, default=60, help=\"Keep companies with batch within N months (best-effort)\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    async with httpx.AsyncClient(headers={\"User-Agent\": USER_AGENT}) as client:\n",
        "        all_companies = await fetch_json(client, ALL_COMPANIES_URL)\n",
        "\n",
        "    if not isinstance(all_companies, list):\n",
        "        raise RuntimeError(\"Unexpected payload from ALL_COMPANIES_URL\")\n",
        "\n",
        "    # 1) Filter (lightweight) + pre-score\n",
        "    candidates: List[Tuple[int, str, Dict[str, Any]]] = []\n",
        "    for c in all_companies:\n",
        "        if args.active_only and (c.get(\"status\") or \"\").lower() != \"active\":\n",
        "            continue\n",
        "\n",
        "        bdt = parse_batch_to_date(c.get(\"batch\") or \"\")\n",
        "        ms = months_since(bdt)\n",
        "        if ms is not None and ms > args.recent_batches_months:\n",
        "            continue\n",
        "\n",
        "        if args.icp_only and not icp_filter_ok(c, min_bucket_hits=args.min_icp_hits):\n",
        "            continue\n",
        "\n",
        "        p, reason = pre_points(c)\n",
        "        candidates.append((p, reason, c))\n",
        "\n",
        "    # Highest pre-score first\n",
        "    candidates.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # 2) Enrich top-N candidates with YC page scrape (founders + latest news)\n",
        "    top_candidates = [c for _, __, c in candidates[: args.prefetch]]\n",
        "    enriched = await enrich_batch(\n",
        "        top_candidates,\n",
        "        concurrency=args.concurrency,\n",
        "        sleep_min=args.sleep_min,\n",
        "        sleep_max=args.sleep_max,\n",
        "    )\n",
        "\n",
        "    # 3) Flatten + final filter\n",
        "    rows = []\n",
        "    for c in enriched:\n",
        "        row = flatten_row(c)\n",
        "        if args.funded_only and not row[\"raised_funding_signal\"]:\n",
        "            continue\n",
        "        rows.append(row)\n",
        "        if len(rows) >= args.limit:\n",
        "            break\n",
        "\n",
        "    if not rows:\n",
        "        raise RuntimeError(\"No rows produced. Try increasing --prefetch or disabling --funded-only.\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.sort_values(by=[\"points\", \"raised_funding_signal\", \"is_hiring\"], ascending=[False, False, False], inplace=True)\n",
        "\n",
        "    with pd.ExcelWriter(args.out, engine=\"openpyxl\") as w:\n",
        "        df.to_excel(w, index=False, sheet_name=\"companies\")\n",
        "\n",
        "    print(f\"Done. Exported {len(df)} rows to: {args.out}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ]
    }
  ]
}